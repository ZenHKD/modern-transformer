Modern Transformer Model Summary
================================================================================

Configuration:
  d_model: 2048
  n_layers: 12
  n_heads: 16
  n_kv_heads: 4
  d_ff: 5632
  max_seq_len: 4096
  sliding_window: 4096
  dropout: 0.0
  drop_path: 0.0
  init_std: 0.02
  use_bias: False
  num_experts: 1
  top_k: 1
  capacity_factor: 1.2
  num_shared_experts: 2
  aux_loss_coef: 0.01
  router_z_loss_coef: 0.001
  noise_std: 0.1

================================================================================

  architecture: encoder_decoder
  vocab_size: 128256
  weight_tying: True
  apply_rope: True
  rope_theta: 500000.0
  rope_fraction: 1.0
  rope_scaling: None
  rope_in_cross_attn: False
  qk_norm: True
  logit_cap: 30.0
  cache_mode: static
  num_sink_tokens: 4
  use_gradient_checkpointing: True
  use_bf16: True
  encoder: BlockConfig(d_model=2048, n_layers=12, n_heads=16, n_kv_heads=4, d_ff=5632, max_seq_len=4096, sliding_window=4096, dropout=0.0, drop_path=0.0, init_std=0.02, use_bias=False, num_experts=1, top_k=1, capacity_factor=1.2, num_shared_experts=2, aux_loss_coef=0.01, router_z_loss_coef=0.001, noise_std=0.1)
  decoder: BlockConfig(d_model=2048, n_layers=12, n_heads=16, n_kv_heads=4, d_ff=5632, max_seq_len=4096, sliding_window=4096, dropout=0.0, drop_path=0.0, init_std=0.02, use_bias=False, num_experts=1, top_k=1, capacity_factor=1.2, num_shared_experts=2, aux_loss_coef=0.01, router_z_loss_coef=0.001, noise_std=0.1)

================================================================================

Architecture:
--------------------------------------------------------------------------------
TransformerModel(
  (embed_tokens): Embedding(128256, 2048)
  (encoder): TransformerEncoder(
    (embed_tokens): Embedding(128256, 2048)
    (layers): ModuleList(
      (0-11): 12 x EncoderBlock(
        (attention): AttentionBlock(
          (wq): Linear(in_features=2048, out_features=2048, bias=False)
          (wk): Linear(in_features=2048, out_features=512, bias=False)
          (wv): Linear(in_features=2048, out_features=512, bias=False)
          (wo): Linear(in_features=2048, out_features=2048, bias=False)
          (q_norm): RMSNorm()
          (k_norm): RMSNorm()
          (rope): RotaryPositionalEmbedding()
        )
        (attention_norm): RMSNorm()
        (feed_forward): ExpertChoiceMoE(
          (gate): Linear(in_features=2048, out_features=1, bias=False)
          (experts): ModuleList(
            (0): SwiGLU(
              (w1): Linear(in_features=2048, out_features=5632, bias=False)
              (w2): Linear(in_features=2048, out_features=5632, bias=False)
              (w3): Linear(in_features=5632, out_features=2048, bias=False)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (shared_experts): ModuleList(
            (0-1): 2 x SwiGLU(
              (w1): Linear(in_features=2048, out_features=2816, bias=False)
              (w2): Linear(in_features=2048, out_features=2816, bias=False)
              (w3): Linear(in_features=2816, out_features=2048, bias=False)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (ffn_norm): RMSNorm()
        (drop_path): Identity()
      )
    )
    (norm): RMSNorm()
  )
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(128256, 2048)
    (layers): ModuleList(
      (0-11): 12 x DecoderBlock(
        (self_attention): AttentionBlock(
          (wq): Linear(in_features=2048, out_features=2048, bias=False)
          (wk): Linear(in_features=2048, out_features=512, bias=False)
          (wv): Linear(in_features=2048, out_features=512, bias=False)
          (wo): Linear(in_features=2048, out_features=2048, bias=False)
          (q_norm): RMSNorm()
          (k_norm): RMSNorm()
          (rope): RotaryPositionalEmbedding()
        )
        (self_attention_norm): RMSNorm()
        (cross_attention): AttentionBlock(
          (wq): Linear(in_features=2048, out_features=2048, bias=False)
          (wk): Linear(in_features=2048, out_features=512, bias=False)
          (wv): Linear(in_features=2048, out_features=512, bias=False)
          (wo): Linear(in_features=2048, out_features=2048, bias=False)
          (q_norm): RMSNorm()
          (k_norm): RMSNorm()
        )
        (cross_attention_norm): RMSNorm()
        (feed_forward): ExpertChoiceMoE(
          (gate): Linear(in_features=2048, out_features=1, bias=False)
          (experts): ModuleList(
            (0): SwiGLU(
              (w1): Linear(in_features=2048, out_features=5632, bias=False)
              (w2): Linear(in_features=2048, out_features=5632, bias=False)
              (w3): Linear(in_features=5632, out_features=2048, bias=False)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (shared_experts): ModuleList(
            (0-1): 2 x SwiGLU(
              (w1): Linear(in_features=2048, out_features=2816, bias=False)
              (w2): Linear(in_features=2048, out_features=2816, bias=False)
              (w3): Linear(in_features=2816, out_features=2048, bias=False)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (ffn_norm): RMSNorm()
        (drop_path): Identity()
      )
    )
    (norm): RMSNorm()
  )
  (head): Linear(in_features=2048, out_features=128256, bias=False)
)

================================================================================

Trainable Parameters per Layer:
Layer Name                                                             |      Parameters
------------------------------------------------------------------------------------------
embed_tokens.weight                                                    |     262,668,288
encoder.layers.0.attention.wq.weight                                   |       4,194,304
encoder.layers.0.attention.wk.weight                                   |       1,048,576
encoder.layers.0.attention.wv.weight                                   |       1,048,576
encoder.layers.0.attention.wo.weight                                   |       4,194,304
encoder.layers.0.attention.q_norm.weight                               |             128
encoder.layers.0.attention.k_norm.weight                               |             128
encoder.layers.0.attention_norm.weight                                 |           2,048
encoder.layers.0.feed_forward.gate.weight                              |           2,048
encoder.layers.0.feed_forward.experts.0.w1.weight                      |      11,534,336
encoder.layers.0.feed_forward.experts.0.w2.weight                      |      11,534,336
encoder.layers.0.feed_forward.experts.0.w3.weight                      |      11,534,336
encoder.layers.0.feed_forward.shared_experts.0.w1.weight               |       5,767,168
encoder.layers.0.feed_forward.shared_experts.0.w2.weight               |       5,767,168
encoder.layers.0.feed_forward.shared_experts.0.w3.weight               |       5,767,168
encoder.layers.0.feed_forward.shared_experts.1.w1.weight               |       5,767,168
encoder.layers.0.feed_forward.shared_experts.1.w2.weight               |       5,767,168
encoder.layers.0.feed_forward.shared_experts.1.w3.weight               |       5,767,168
encoder.layers.0.ffn_norm.weight                                       |           2,048
encoder.layers.1.attention.wq.weight                                   |       4,194,304
encoder.layers.1.attention.wk.weight                                   |       1,048,576
encoder.layers.1.attention.wv.weight                                   |       1,048,576
encoder.layers.1.attention.wo.weight                                   |       4,194,304
encoder.layers.1.attention.q_norm.weight                               |             128
encoder.layers.1.attention.k_norm.weight                               |             128
encoder.layers.1.attention_norm.weight                                 |           2,048
encoder.layers.1.feed_forward.gate.weight                              |           2,048
encoder.layers.1.feed_forward.experts.0.w1.weight                      |      11,534,336
encoder.layers.1.feed_forward.experts.0.w2.weight                      |      11,534,336
encoder.layers.1.feed_forward.experts.0.w3.weight                      |      11,534,336
encoder.layers.1.feed_forward.shared_experts.0.w1.weight               |       5,767,168
encoder.layers.1.feed_forward.shared_experts.0.w2.weight               |       5,767,168
encoder.layers.1.feed_forward.shared_experts.0.w3.weight               |       5,767,168
encoder.layers.1.feed_forward.shared_experts.1.w1.weight               |       5,767,168
encoder.layers.1.feed_forward.shared_experts.1.w2.weight               |       5,767,168
encoder.layers.1.feed_forward.shared_experts.1.w3.weight               |       5,767,168
encoder.layers.1.ffn_norm.weight                                       |           2,048
encoder.layers.2.attention.wq.weight                                   |       4,194,304
encoder.layers.2.attention.wk.weight                                   |       1,048,576
encoder.layers.2.attention.wv.weight                                   |       1,048,576
encoder.layers.2.attention.wo.weight                                   |       4,194,304
encoder.layers.2.attention.q_norm.weight                               |             128
encoder.layers.2.attention.k_norm.weight                               |             128
encoder.layers.2.attention_norm.weight                                 |           2,048
encoder.layers.2.feed_forward.gate.weight                              |           2,048
encoder.layers.2.feed_forward.experts.0.w1.weight                      |      11,534,336
encoder.layers.2.feed_forward.experts.0.w2.weight                      |      11,534,336
encoder.layers.2.feed_forward.experts.0.w3.weight                      |      11,534,336
encoder.layers.2.feed_forward.shared_experts.0.w1.weight               |       5,767,168
encoder.layers.2.feed_forward.shared_experts.0.w2.weight               |       5,767,168
encoder.layers.2.feed_forward.shared_experts.0.w3.weight               |       5,767,168
encoder.layers.2.feed_forward.shared_experts.1.w1.weight               |       5,767,168
encoder.layers.2.feed_forward.shared_experts.1.w2.weight               |       5,767,168
encoder.layers.2.feed_forward.shared_experts.1.w3.weight               |       5,767,168
encoder.layers.2.ffn_norm.weight                                       |           2,048
encoder.layers.3.attention.wq.weight                                   |       4,194,304
encoder.layers.3.attention.wk.weight                                   |       1,048,576
encoder.layers.3.attention.wv.weight                                   |       1,048,576
encoder.layers.3.attention.wo.weight                                   |       4,194,304
encoder.layers.3.attention.q_norm.weight                               |             128
encoder.layers.3.attention.k_norm.weight                               |             128
encoder.layers.3.attention_norm.weight                                 |           2,048
encoder.layers.3.feed_forward.gate.weight                              |           2,048
encoder.layers.3.feed_forward.experts.0.w1.weight                      |      11,534,336
encoder.layers.3.feed_forward.experts.0.w2.weight                      |      11,534,336
encoder.layers.3.feed_forward.experts.0.w3.weight                      |      11,534,336
encoder.layers.3.feed_forward.shared_experts.0.w1.weight               |       5,767,168
encoder.layers.3.feed_forward.shared_experts.0.w2.weight               |       5,767,168
encoder.layers.3.feed_forward.shared_experts.0.w3.weight               |       5,767,168
encoder.layers.3.feed_forward.shared_experts.1.w1.weight               |       5,767,168
encoder.layers.3.feed_forward.shared_experts.1.w2.weight               |       5,767,168
encoder.layers.3.feed_forward.shared_experts.1.w3.weight               |       5,767,168
encoder.layers.3.ffn_norm.weight                                       |           2,048
encoder.layers.4.attention.wq.weight                                   |       4,194,304
encoder.layers.4.attention.wk.weight                                   |       1,048,576
encoder.layers.4.attention.wv.weight                                   |       1,048,576
encoder.layers.4.attention.wo.weight                                   |       4,194,304
encoder.layers.4.attention.q_norm.weight                               |             128
encoder.layers.4.attention.k_norm.weight                               |             128
encoder.layers.4.attention_norm.weight                                 |           2,048
encoder.layers.4.feed_forward.gate.weight                              |           2,048
encoder.layers.4.feed_forward.experts.0.w1.weight                      |      11,534,336
encoder.layers.4.feed_forward.experts.0.w2.weight                      |      11,534,336
encoder.layers.4.feed_forward.experts.0.w3.weight                      |      11,534,336
encoder.layers.4.feed_forward.shared_experts.0.w1.weight               |       5,767,168
encoder.layers.4.feed_forward.shared_experts.0.w2.weight               |       5,767,168
encoder.layers.4.feed_forward.shared_experts.0.w3.weight               |       5,767,168
encoder.layers.4.feed_forward.shared_experts.1.w1.weight               |       5,767,168
encoder.layers.4.feed_forward.shared_experts.1.w2.weight               |       5,767,168
encoder.layers.4.feed_forward.shared_experts.1.w3.weight               |       5,767,168
encoder.layers.4.ffn_norm.weight                                       |           2,048
encoder.layers.5.attention.wq.weight                                   |       4,194,304
encoder.layers.5.attention.wk.weight                                   |       1,048,576
encoder.layers.5.attention.wv.weight                                   |       1,048,576
encoder.layers.5.attention.wo.weight                                   |       4,194,304
encoder.layers.5.attention.q_norm.weight                               |             128
encoder.layers.5.attention.k_norm.weight                               |             128
encoder.layers.5.attention_norm.weight                                 |           2,048
encoder.layers.5.feed_forward.gate.weight                              |           2,048
encoder.layers.5.feed_forward.experts.0.w1.weight                      |      11,534,336
encoder.layers.5.feed_forward.experts.0.w2.weight                      |      11,534,336
encoder.layers.5.feed_forward.experts.0.w3.weight                      |      11,534,336
encoder.layers.5.feed_forward.shared_experts.0.w1.weight               |       5,767,168
encoder.layers.5.feed_forward.shared_experts.0.w2.weight               |       5,767,168
encoder.layers.5.feed_forward.shared_experts.0.w3.weight               |       5,767,168
encoder.layers.5.feed_forward.shared_experts.1.w1.weight               |       5,767,168
encoder.layers.5.feed_forward.shared_experts.1.w2.weight               |       5,767,168
encoder.layers.5.feed_forward.shared_experts.1.w3.weight               |       5,767,168
encoder.layers.5.ffn_norm.weight                                       |           2,048
encoder.layers.6.attention.wq.weight                                   |       4,194,304
encoder.layers.6.attention.wk.weight                                   |       1,048,576
encoder.layers.6.attention.wv.weight                                   |       1,048,576
encoder.layers.6.attention.wo.weight                                   |       4,194,304
encoder.layers.6.attention.q_norm.weight                               |             128
encoder.layers.6.attention.k_norm.weight                               |             128
encoder.layers.6.attention_norm.weight                                 |           2,048
encoder.layers.6.feed_forward.gate.weight                              |           2,048
encoder.layers.6.feed_forward.experts.0.w1.weight                      |      11,534,336
encoder.layers.6.feed_forward.experts.0.w2.weight                      |      11,534,336
encoder.layers.6.feed_forward.experts.0.w3.weight                      |      11,534,336
encoder.layers.6.feed_forward.shared_experts.0.w1.weight               |       5,767,168
encoder.layers.6.feed_forward.shared_experts.0.w2.weight               |       5,767,168
encoder.layers.6.feed_forward.shared_experts.0.w3.weight               |       5,767,168
encoder.layers.6.feed_forward.shared_experts.1.w1.weight               |       5,767,168
encoder.layers.6.feed_forward.shared_experts.1.w2.weight               |       5,767,168
encoder.layers.6.feed_forward.shared_experts.1.w3.weight               |       5,767,168
encoder.layers.6.ffn_norm.weight                                       |           2,048
encoder.layers.7.attention.wq.weight                                   |       4,194,304
encoder.layers.7.attention.wk.weight                                   |       1,048,576
encoder.layers.7.attention.wv.weight                                   |       1,048,576
encoder.layers.7.attention.wo.weight                                   |       4,194,304
encoder.layers.7.attention.q_norm.weight                               |             128
encoder.layers.7.attention.k_norm.weight                               |             128
encoder.layers.7.attention_norm.weight                                 |           2,048
encoder.layers.7.feed_forward.gate.weight                              |           2,048
encoder.layers.7.feed_forward.experts.0.w1.weight                      |      11,534,336
encoder.layers.7.feed_forward.experts.0.w2.weight                      |      11,534,336
encoder.layers.7.feed_forward.experts.0.w3.weight                      |      11,534,336
encoder.layers.7.feed_forward.shared_experts.0.w1.weight               |       5,767,168
encoder.layers.7.feed_forward.shared_experts.0.w2.weight               |       5,767,168
encoder.layers.7.feed_forward.shared_experts.0.w3.weight               |       5,767,168
encoder.layers.7.feed_forward.shared_experts.1.w1.weight               |       5,767,168
encoder.layers.7.feed_forward.shared_experts.1.w2.weight               |       5,767,168
encoder.layers.7.feed_forward.shared_experts.1.w3.weight               |       5,767,168
encoder.layers.7.ffn_norm.weight                                       |           2,048
encoder.layers.8.attention.wq.weight                                   |       4,194,304
encoder.layers.8.attention.wk.weight                                   |       1,048,576
encoder.layers.8.attention.wv.weight                                   |       1,048,576
encoder.layers.8.attention.wo.weight                                   |       4,194,304
encoder.layers.8.attention.q_norm.weight                               |             128
encoder.layers.8.attention.k_norm.weight                               |             128
encoder.layers.8.attention_norm.weight                                 |           2,048
encoder.layers.8.feed_forward.gate.weight                              |           2,048
encoder.layers.8.feed_forward.experts.0.w1.weight                      |      11,534,336
encoder.layers.8.feed_forward.experts.0.w2.weight                      |      11,534,336
encoder.layers.8.feed_forward.experts.0.w3.weight                      |      11,534,336
encoder.layers.8.feed_forward.shared_experts.0.w1.weight               |       5,767,168
encoder.layers.8.feed_forward.shared_experts.0.w2.weight               |       5,767,168
encoder.layers.8.feed_forward.shared_experts.0.w3.weight               |       5,767,168
encoder.layers.8.feed_forward.shared_experts.1.w1.weight               |       5,767,168
encoder.layers.8.feed_forward.shared_experts.1.w2.weight               |       5,767,168
encoder.layers.8.feed_forward.shared_experts.1.w3.weight               |       5,767,168
encoder.layers.8.ffn_norm.weight                                       |           2,048
encoder.layers.9.attention.wq.weight                                   |       4,194,304
encoder.layers.9.attention.wk.weight                                   |       1,048,576
encoder.layers.9.attention.wv.weight                                   |       1,048,576
encoder.layers.9.attention.wo.weight                                   |       4,194,304
encoder.layers.9.attention.q_norm.weight                               |             128
encoder.layers.9.attention.k_norm.weight                               |             128
encoder.layers.9.attention_norm.weight                                 |           2,048
encoder.layers.9.feed_forward.gate.weight                              |           2,048
encoder.layers.9.feed_forward.experts.0.w1.weight                      |      11,534,336
encoder.layers.9.feed_forward.experts.0.w2.weight                      |      11,534,336
encoder.layers.9.feed_forward.experts.0.w3.weight                      |      11,534,336
encoder.layers.9.feed_forward.shared_experts.0.w1.weight               |       5,767,168
encoder.layers.9.feed_forward.shared_experts.0.w2.weight               |       5,767,168
encoder.layers.9.feed_forward.shared_experts.0.w3.weight               |       5,767,168
encoder.layers.9.feed_forward.shared_experts.1.w1.weight               |       5,767,168
encoder.layers.9.feed_forward.shared_experts.1.w2.weight               |       5,767,168
encoder.layers.9.feed_forward.shared_experts.1.w3.weight               |       5,767,168
encoder.layers.9.ffn_norm.weight                                       |           2,048
encoder.layers.10.attention.wq.weight                                  |       4,194,304
encoder.layers.10.attention.wk.weight                                  |       1,048,576
encoder.layers.10.attention.wv.weight                                  |       1,048,576
encoder.layers.10.attention.wo.weight                                  |       4,194,304
encoder.layers.10.attention.q_norm.weight                              |             128
encoder.layers.10.attention.k_norm.weight                              |             128
encoder.layers.10.attention_norm.weight                                |           2,048
encoder.layers.10.feed_forward.gate.weight                             |           2,048
encoder.layers.10.feed_forward.experts.0.w1.weight                     |      11,534,336
encoder.layers.10.feed_forward.experts.0.w2.weight                     |      11,534,336
encoder.layers.10.feed_forward.experts.0.w3.weight                     |      11,534,336
encoder.layers.10.feed_forward.shared_experts.0.w1.weight              |       5,767,168
encoder.layers.10.feed_forward.shared_experts.0.w2.weight              |       5,767,168
encoder.layers.10.feed_forward.shared_experts.0.w3.weight              |       5,767,168
encoder.layers.10.feed_forward.shared_experts.1.w1.weight              |       5,767,168
encoder.layers.10.feed_forward.shared_experts.1.w2.weight              |       5,767,168
encoder.layers.10.feed_forward.shared_experts.1.w3.weight              |       5,767,168
encoder.layers.10.ffn_norm.weight                                      |           2,048
encoder.layers.11.attention.wq.weight                                  |       4,194,304
encoder.layers.11.attention.wk.weight                                  |       1,048,576
encoder.layers.11.attention.wv.weight                                  |       1,048,576
encoder.layers.11.attention.wo.weight                                  |       4,194,304
encoder.layers.11.attention.q_norm.weight                              |             128
encoder.layers.11.attention.k_norm.weight                              |             128
encoder.layers.11.attention_norm.weight                                |           2,048
encoder.layers.11.feed_forward.gate.weight                             |           2,048
encoder.layers.11.feed_forward.experts.0.w1.weight                     |      11,534,336
encoder.layers.11.feed_forward.experts.0.w2.weight                     |      11,534,336
encoder.layers.11.feed_forward.experts.0.w3.weight                     |      11,534,336
encoder.layers.11.feed_forward.shared_experts.0.w1.weight              |       5,767,168
encoder.layers.11.feed_forward.shared_experts.0.w2.weight              |       5,767,168
encoder.layers.11.feed_forward.shared_experts.0.w3.weight              |       5,767,168
encoder.layers.11.feed_forward.shared_experts.1.w1.weight              |       5,767,168
encoder.layers.11.feed_forward.shared_experts.1.w2.weight              |       5,767,168
encoder.layers.11.feed_forward.shared_experts.1.w3.weight              |       5,767,168
encoder.layers.11.ffn_norm.weight                                      |           2,048
encoder.norm.weight                                                    |           2,048
decoder.layers.0.self_attention.wq.weight                              |       4,194,304
decoder.layers.0.self_attention.wk.weight                              |       1,048,576
decoder.layers.0.self_attention.wv.weight                              |       1,048,576
decoder.layers.0.self_attention.wo.weight                              |       4,194,304
decoder.layers.0.self_attention.q_norm.weight                          |             128
decoder.layers.0.self_attention.k_norm.weight                          |             128
decoder.layers.0.self_attention_norm.weight                            |           2,048
decoder.layers.0.cross_attention.wq.weight                             |       4,194,304
decoder.layers.0.cross_attention.wk.weight                             |       1,048,576
decoder.layers.0.cross_attention.wv.weight                             |       1,048,576
decoder.layers.0.cross_attention.wo.weight                             |       4,194,304
decoder.layers.0.cross_attention.q_norm.weight                         |             128
decoder.layers.0.cross_attention.k_norm.weight                         |             128
decoder.layers.0.cross_attention_norm.weight                           |           2,048
decoder.layers.0.feed_forward.gate.weight                              |           2,048
decoder.layers.0.feed_forward.experts.0.w1.weight                      |      11,534,336
decoder.layers.0.feed_forward.experts.0.w2.weight                      |      11,534,336
decoder.layers.0.feed_forward.experts.0.w3.weight                      |      11,534,336
decoder.layers.0.feed_forward.shared_experts.0.w1.weight               |       5,767,168
decoder.layers.0.feed_forward.shared_experts.0.w2.weight               |       5,767,168
decoder.layers.0.feed_forward.shared_experts.0.w3.weight               |       5,767,168
decoder.layers.0.feed_forward.shared_experts.1.w1.weight               |       5,767,168
decoder.layers.0.feed_forward.shared_experts.1.w2.weight               |       5,767,168
decoder.layers.0.feed_forward.shared_experts.1.w3.weight               |       5,767,168
decoder.layers.0.ffn_norm.weight                                       |           2,048
decoder.layers.1.self_attention.wq.weight                              |       4,194,304
decoder.layers.1.self_attention.wk.weight                              |       1,048,576
decoder.layers.1.self_attention.wv.weight                              |       1,048,576
decoder.layers.1.self_attention.wo.weight                              |       4,194,304
decoder.layers.1.self_attention.q_norm.weight                          |             128
decoder.layers.1.self_attention.k_norm.weight                          |             128
decoder.layers.1.self_attention_norm.weight                            |           2,048
decoder.layers.1.cross_attention.wq.weight                             |       4,194,304
decoder.layers.1.cross_attention.wk.weight                             |       1,048,576
decoder.layers.1.cross_attention.wv.weight                             |       1,048,576
decoder.layers.1.cross_attention.wo.weight                             |       4,194,304
decoder.layers.1.cross_attention.q_norm.weight                         |             128
decoder.layers.1.cross_attention.k_norm.weight                         |             128
decoder.layers.1.cross_attention_norm.weight                           |           2,048
decoder.layers.1.feed_forward.gate.weight                              |           2,048
decoder.layers.1.feed_forward.experts.0.w1.weight                      |      11,534,336
decoder.layers.1.feed_forward.experts.0.w2.weight                      |      11,534,336
decoder.layers.1.feed_forward.experts.0.w3.weight                      |      11,534,336
decoder.layers.1.feed_forward.shared_experts.0.w1.weight               |       5,767,168
decoder.layers.1.feed_forward.shared_experts.0.w2.weight               |       5,767,168
decoder.layers.1.feed_forward.shared_experts.0.w3.weight               |       5,767,168
decoder.layers.1.feed_forward.shared_experts.1.w1.weight               |       5,767,168
decoder.layers.1.feed_forward.shared_experts.1.w2.weight               |       5,767,168
decoder.layers.1.feed_forward.shared_experts.1.w3.weight               |       5,767,168
decoder.layers.1.ffn_norm.weight                                       |           2,048
decoder.layers.2.self_attention.wq.weight                              |       4,194,304
decoder.layers.2.self_attention.wk.weight                              |       1,048,576
decoder.layers.2.self_attention.wv.weight                              |       1,048,576
decoder.layers.2.self_attention.wo.weight                              |       4,194,304
decoder.layers.2.self_attention.q_norm.weight                          |             128
decoder.layers.2.self_attention.k_norm.weight                          |             128
decoder.layers.2.self_attention_norm.weight                            |           2,048
decoder.layers.2.cross_attention.wq.weight                             |       4,194,304
decoder.layers.2.cross_attention.wk.weight                             |       1,048,576
decoder.layers.2.cross_attention.wv.weight                             |       1,048,576
decoder.layers.2.cross_attention.wo.weight                             |       4,194,304
decoder.layers.2.cross_attention.q_norm.weight                         |             128
decoder.layers.2.cross_attention.k_norm.weight                         |             128
decoder.layers.2.cross_attention_norm.weight                           |           2,048
decoder.layers.2.feed_forward.gate.weight                              |           2,048
decoder.layers.2.feed_forward.experts.0.w1.weight                      |      11,534,336
decoder.layers.2.feed_forward.experts.0.w2.weight                      |      11,534,336
decoder.layers.2.feed_forward.experts.0.w3.weight                      |      11,534,336
decoder.layers.2.feed_forward.shared_experts.0.w1.weight               |       5,767,168
decoder.layers.2.feed_forward.shared_experts.0.w2.weight               |       5,767,168
decoder.layers.2.feed_forward.shared_experts.0.w3.weight               |       5,767,168
decoder.layers.2.feed_forward.shared_experts.1.w1.weight               |       5,767,168
decoder.layers.2.feed_forward.shared_experts.1.w2.weight               |       5,767,168
decoder.layers.2.feed_forward.shared_experts.1.w3.weight               |       5,767,168
decoder.layers.2.ffn_norm.weight                                       |           2,048
decoder.layers.3.self_attention.wq.weight                              |       4,194,304
decoder.layers.3.self_attention.wk.weight                              |       1,048,576
decoder.layers.3.self_attention.wv.weight                              |       1,048,576
decoder.layers.3.self_attention.wo.weight                              |       4,194,304
decoder.layers.3.self_attention.q_norm.weight                          |             128
decoder.layers.3.self_attention.k_norm.weight                          |             128
decoder.layers.3.self_attention_norm.weight                            |           2,048
decoder.layers.3.cross_attention.wq.weight                             |       4,194,304
decoder.layers.3.cross_attention.wk.weight                             |       1,048,576
decoder.layers.3.cross_attention.wv.weight                             |       1,048,576
decoder.layers.3.cross_attention.wo.weight                             |       4,194,304
decoder.layers.3.cross_attention.q_norm.weight                         |             128
decoder.layers.3.cross_attention.k_norm.weight                         |             128
decoder.layers.3.cross_attention_norm.weight                           |           2,048
decoder.layers.3.feed_forward.gate.weight                              |           2,048
decoder.layers.3.feed_forward.experts.0.w1.weight                      |      11,534,336
decoder.layers.3.feed_forward.experts.0.w2.weight                      |      11,534,336
decoder.layers.3.feed_forward.experts.0.w3.weight                      |      11,534,336
decoder.layers.3.feed_forward.shared_experts.0.w1.weight               |       5,767,168
decoder.layers.3.feed_forward.shared_experts.0.w2.weight               |       5,767,168
decoder.layers.3.feed_forward.shared_experts.0.w3.weight               |       5,767,168
decoder.layers.3.feed_forward.shared_experts.1.w1.weight               |       5,767,168
decoder.layers.3.feed_forward.shared_experts.1.w2.weight               |       5,767,168
decoder.layers.3.feed_forward.shared_experts.1.w3.weight               |       5,767,168
decoder.layers.3.ffn_norm.weight                                       |           2,048
decoder.layers.4.self_attention.wq.weight                              |       4,194,304
decoder.layers.4.self_attention.wk.weight                              |       1,048,576
decoder.layers.4.self_attention.wv.weight                              |       1,048,576
decoder.layers.4.self_attention.wo.weight                              |       4,194,304
decoder.layers.4.self_attention.q_norm.weight                          |             128
decoder.layers.4.self_attention.k_norm.weight                          |             128
decoder.layers.4.self_attention_norm.weight                            |           2,048
decoder.layers.4.cross_attention.wq.weight                             |       4,194,304
decoder.layers.4.cross_attention.wk.weight                             |       1,048,576
decoder.layers.4.cross_attention.wv.weight                             |       1,048,576
decoder.layers.4.cross_attention.wo.weight                             |       4,194,304
decoder.layers.4.cross_attention.q_norm.weight                         |             128
decoder.layers.4.cross_attention.k_norm.weight                         |             128
decoder.layers.4.cross_attention_norm.weight                           |           2,048
decoder.layers.4.feed_forward.gate.weight                              |           2,048
decoder.layers.4.feed_forward.experts.0.w1.weight                      |      11,534,336
decoder.layers.4.feed_forward.experts.0.w2.weight                      |      11,534,336
decoder.layers.4.feed_forward.experts.0.w3.weight                      |      11,534,336
decoder.layers.4.feed_forward.shared_experts.0.w1.weight               |       5,767,168
decoder.layers.4.feed_forward.shared_experts.0.w2.weight               |       5,767,168
decoder.layers.4.feed_forward.shared_experts.0.w3.weight               |       5,767,168
decoder.layers.4.feed_forward.shared_experts.1.w1.weight               |       5,767,168
decoder.layers.4.feed_forward.shared_experts.1.w2.weight               |       5,767,168
decoder.layers.4.feed_forward.shared_experts.1.w3.weight               |       5,767,168
decoder.layers.4.ffn_norm.weight                                       |           2,048
decoder.layers.5.self_attention.wq.weight                              |       4,194,304
decoder.layers.5.self_attention.wk.weight                              |       1,048,576
decoder.layers.5.self_attention.wv.weight                              |       1,048,576
decoder.layers.5.self_attention.wo.weight                              |       4,194,304
decoder.layers.5.self_attention.q_norm.weight                          |             128
decoder.layers.5.self_attention.k_norm.weight                          |             128
decoder.layers.5.self_attention_norm.weight                            |           2,048
decoder.layers.5.cross_attention.wq.weight                             |       4,194,304
decoder.layers.5.cross_attention.wk.weight                             |       1,048,576
decoder.layers.5.cross_attention.wv.weight                             |       1,048,576
decoder.layers.5.cross_attention.wo.weight                             |       4,194,304
decoder.layers.5.cross_attention.q_norm.weight                         |             128
decoder.layers.5.cross_attention.k_norm.weight                         |             128
decoder.layers.5.cross_attention_norm.weight                           |           2,048
decoder.layers.5.feed_forward.gate.weight                              |           2,048
decoder.layers.5.feed_forward.experts.0.w1.weight                      |      11,534,336
decoder.layers.5.feed_forward.experts.0.w2.weight                      |      11,534,336
decoder.layers.5.feed_forward.experts.0.w3.weight                      |      11,534,336
decoder.layers.5.feed_forward.shared_experts.0.w1.weight               |       5,767,168
decoder.layers.5.feed_forward.shared_experts.0.w2.weight               |       5,767,168
decoder.layers.5.feed_forward.shared_experts.0.w3.weight               |       5,767,168
decoder.layers.5.feed_forward.shared_experts.1.w1.weight               |       5,767,168
decoder.layers.5.feed_forward.shared_experts.1.w2.weight               |       5,767,168
decoder.layers.5.feed_forward.shared_experts.1.w3.weight               |       5,767,168
decoder.layers.5.ffn_norm.weight                                       |           2,048
decoder.layers.6.self_attention.wq.weight                              |       4,194,304
decoder.layers.6.self_attention.wk.weight                              |       1,048,576
decoder.layers.6.self_attention.wv.weight                              |       1,048,576
decoder.layers.6.self_attention.wo.weight                              |       4,194,304
decoder.layers.6.self_attention.q_norm.weight                          |             128
decoder.layers.6.self_attention.k_norm.weight                          |             128
decoder.layers.6.self_attention_norm.weight                            |           2,048
decoder.layers.6.cross_attention.wq.weight                             |       4,194,304
decoder.layers.6.cross_attention.wk.weight                             |       1,048,576
decoder.layers.6.cross_attention.wv.weight                             |       1,048,576
decoder.layers.6.cross_attention.wo.weight                             |       4,194,304
decoder.layers.6.cross_attention.q_norm.weight                         |             128
decoder.layers.6.cross_attention.k_norm.weight                         |             128
decoder.layers.6.cross_attention_norm.weight                           |           2,048
decoder.layers.6.feed_forward.gate.weight                              |           2,048
decoder.layers.6.feed_forward.experts.0.w1.weight                      |      11,534,336
decoder.layers.6.feed_forward.experts.0.w2.weight                      |      11,534,336
decoder.layers.6.feed_forward.experts.0.w3.weight                      |      11,534,336
decoder.layers.6.feed_forward.shared_experts.0.w1.weight               |       5,767,168
decoder.layers.6.feed_forward.shared_experts.0.w2.weight               |       5,767,168
decoder.layers.6.feed_forward.shared_experts.0.w3.weight               |       5,767,168
decoder.layers.6.feed_forward.shared_experts.1.w1.weight               |       5,767,168
decoder.layers.6.feed_forward.shared_experts.1.w2.weight               |       5,767,168
decoder.layers.6.feed_forward.shared_experts.1.w3.weight               |       5,767,168
decoder.layers.6.ffn_norm.weight                                       |           2,048
decoder.layers.7.self_attention.wq.weight                              |       4,194,304
decoder.layers.7.self_attention.wk.weight                              |       1,048,576
decoder.layers.7.self_attention.wv.weight                              |       1,048,576
decoder.layers.7.self_attention.wo.weight                              |       4,194,304
decoder.layers.7.self_attention.q_norm.weight                          |             128
decoder.layers.7.self_attention.k_norm.weight                          |             128
decoder.layers.7.self_attention_norm.weight                            |           2,048
decoder.layers.7.cross_attention.wq.weight                             |       4,194,304
decoder.layers.7.cross_attention.wk.weight                             |       1,048,576
decoder.layers.7.cross_attention.wv.weight                             |       1,048,576
decoder.layers.7.cross_attention.wo.weight                             |       4,194,304
decoder.layers.7.cross_attention.q_norm.weight                         |             128
decoder.layers.7.cross_attention.k_norm.weight                         |             128
decoder.layers.7.cross_attention_norm.weight                           |           2,048
decoder.layers.7.feed_forward.gate.weight                              |           2,048
decoder.layers.7.feed_forward.experts.0.w1.weight                      |      11,534,336
decoder.layers.7.feed_forward.experts.0.w2.weight                      |      11,534,336
decoder.layers.7.feed_forward.experts.0.w3.weight                      |      11,534,336
decoder.layers.7.feed_forward.shared_experts.0.w1.weight               |       5,767,168
decoder.layers.7.feed_forward.shared_experts.0.w2.weight               |       5,767,168
decoder.layers.7.feed_forward.shared_experts.0.w3.weight               |       5,767,168
decoder.layers.7.feed_forward.shared_experts.1.w1.weight               |       5,767,168
decoder.layers.7.feed_forward.shared_experts.1.w2.weight               |       5,767,168
decoder.layers.7.feed_forward.shared_experts.1.w3.weight               |       5,767,168
decoder.layers.7.ffn_norm.weight                                       |           2,048
decoder.layers.8.self_attention.wq.weight                              |       4,194,304
decoder.layers.8.self_attention.wk.weight                              |       1,048,576
decoder.layers.8.self_attention.wv.weight                              |       1,048,576
decoder.layers.8.self_attention.wo.weight                              |       4,194,304
decoder.layers.8.self_attention.q_norm.weight                          |             128
decoder.layers.8.self_attention.k_norm.weight                          |             128
decoder.layers.8.self_attention_norm.weight                            |           2,048
decoder.layers.8.cross_attention.wq.weight                             |       4,194,304
decoder.layers.8.cross_attention.wk.weight                             |       1,048,576
decoder.layers.8.cross_attention.wv.weight                             |       1,048,576
decoder.layers.8.cross_attention.wo.weight                             |       4,194,304
decoder.layers.8.cross_attention.q_norm.weight                         |             128
decoder.layers.8.cross_attention.k_norm.weight                         |             128
decoder.layers.8.cross_attention_norm.weight                           |           2,048
decoder.layers.8.feed_forward.gate.weight                              |           2,048
decoder.layers.8.feed_forward.experts.0.w1.weight                      |      11,534,336
decoder.layers.8.feed_forward.experts.0.w2.weight                      |      11,534,336
decoder.layers.8.feed_forward.experts.0.w3.weight                      |      11,534,336
decoder.layers.8.feed_forward.shared_experts.0.w1.weight               |       5,767,168
decoder.layers.8.feed_forward.shared_experts.0.w2.weight               |       5,767,168
decoder.layers.8.feed_forward.shared_experts.0.w3.weight               |       5,767,168
decoder.layers.8.feed_forward.shared_experts.1.w1.weight               |       5,767,168
decoder.layers.8.feed_forward.shared_experts.1.w2.weight               |       5,767,168
decoder.layers.8.feed_forward.shared_experts.1.w3.weight               |       5,767,168
decoder.layers.8.ffn_norm.weight                                       |           2,048
decoder.layers.9.self_attention.wq.weight                              |       4,194,304
decoder.layers.9.self_attention.wk.weight                              |       1,048,576
decoder.layers.9.self_attention.wv.weight                              |       1,048,576
decoder.layers.9.self_attention.wo.weight                              |       4,194,304
decoder.layers.9.self_attention.q_norm.weight                          |             128
decoder.layers.9.self_attention.k_norm.weight                          |             128
decoder.layers.9.self_attention_norm.weight                            |           2,048
decoder.layers.9.cross_attention.wq.weight                             |       4,194,304
decoder.layers.9.cross_attention.wk.weight                             |       1,048,576
decoder.layers.9.cross_attention.wv.weight                             |       1,048,576
decoder.layers.9.cross_attention.wo.weight                             |       4,194,304
decoder.layers.9.cross_attention.q_norm.weight                         |             128
decoder.layers.9.cross_attention.k_norm.weight                         |             128
decoder.layers.9.cross_attention_norm.weight                           |           2,048
decoder.layers.9.feed_forward.gate.weight                              |           2,048
decoder.layers.9.feed_forward.experts.0.w1.weight                      |      11,534,336
decoder.layers.9.feed_forward.experts.0.w2.weight                      |      11,534,336
decoder.layers.9.feed_forward.experts.0.w3.weight                      |      11,534,336
decoder.layers.9.feed_forward.shared_experts.0.w1.weight               |       5,767,168
decoder.layers.9.feed_forward.shared_experts.0.w2.weight               |       5,767,168
decoder.layers.9.feed_forward.shared_experts.0.w3.weight               |       5,767,168
decoder.layers.9.feed_forward.shared_experts.1.w1.weight               |       5,767,168
decoder.layers.9.feed_forward.shared_experts.1.w2.weight               |       5,767,168
decoder.layers.9.feed_forward.shared_experts.1.w3.weight               |       5,767,168
decoder.layers.9.ffn_norm.weight                                       |           2,048
decoder.layers.10.self_attention.wq.weight                             |       4,194,304
decoder.layers.10.self_attention.wk.weight                             |       1,048,576
decoder.layers.10.self_attention.wv.weight                             |       1,048,576
decoder.layers.10.self_attention.wo.weight                             |       4,194,304
decoder.layers.10.self_attention.q_norm.weight                         |             128
decoder.layers.10.self_attention.k_norm.weight                         |             128
decoder.layers.10.self_attention_norm.weight                           |           2,048
decoder.layers.10.cross_attention.wq.weight                            |       4,194,304
decoder.layers.10.cross_attention.wk.weight                            |       1,048,576
decoder.layers.10.cross_attention.wv.weight                            |       1,048,576
decoder.layers.10.cross_attention.wo.weight                            |       4,194,304
decoder.layers.10.cross_attention.q_norm.weight                        |             128
decoder.layers.10.cross_attention.k_norm.weight                        |             128
decoder.layers.10.cross_attention_norm.weight                          |           2,048
decoder.layers.10.feed_forward.gate.weight                             |           2,048
decoder.layers.10.feed_forward.experts.0.w1.weight                     |      11,534,336
decoder.layers.10.feed_forward.experts.0.w2.weight                     |      11,534,336
decoder.layers.10.feed_forward.experts.0.w3.weight                     |      11,534,336
decoder.layers.10.feed_forward.shared_experts.0.w1.weight              |       5,767,168
decoder.layers.10.feed_forward.shared_experts.0.w2.weight              |       5,767,168
decoder.layers.10.feed_forward.shared_experts.0.w3.weight              |       5,767,168
decoder.layers.10.feed_forward.shared_experts.1.w1.weight              |       5,767,168
decoder.layers.10.feed_forward.shared_experts.1.w2.weight              |       5,767,168
decoder.layers.10.feed_forward.shared_experts.1.w3.weight              |       5,767,168
decoder.layers.10.ffn_norm.weight                                      |           2,048
decoder.layers.11.self_attention.wq.weight                             |       4,194,304
decoder.layers.11.self_attention.wk.weight                             |       1,048,576
decoder.layers.11.self_attention.wv.weight                             |       1,048,576
decoder.layers.11.self_attention.wo.weight                             |       4,194,304
decoder.layers.11.self_attention.q_norm.weight                         |             128
decoder.layers.11.self_attention.k_norm.weight                         |             128
decoder.layers.11.self_attention_norm.weight                           |           2,048
decoder.layers.11.cross_attention.wq.weight                            |       4,194,304
decoder.layers.11.cross_attention.wk.weight                            |       1,048,576
decoder.layers.11.cross_attention.wv.weight                            |       1,048,576
decoder.layers.11.cross_attention.wo.weight                            |       4,194,304
decoder.layers.11.cross_attention.q_norm.weight                        |             128
decoder.layers.11.cross_attention.k_norm.weight                        |             128
decoder.layers.11.cross_attention_norm.weight                          |           2,048
decoder.layers.11.feed_forward.gate.weight                             |           2,048
decoder.layers.11.feed_forward.experts.0.w1.weight                     |      11,534,336
decoder.layers.11.feed_forward.experts.0.w2.weight                     |      11,534,336
decoder.layers.11.feed_forward.experts.0.w3.weight                     |      11,534,336
decoder.layers.11.feed_forward.shared_experts.0.w1.weight              |       5,767,168
decoder.layers.11.feed_forward.shared_experts.0.w2.weight              |       5,767,168
decoder.layers.11.feed_forward.shared_experts.0.w3.weight              |       5,767,168
decoder.layers.11.feed_forward.shared_experts.1.w1.weight              |       5,767,168
decoder.layers.11.feed_forward.shared_experts.1.w2.weight              |       5,767,168
decoder.layers.11.feed_forward.shared_experts.1.w3.weight              |       5,767,168
decoder.layers.11.ffn_norm.weight                                      |           2,048
decoder.norm.weight                                                    |           2,048
==========================================================================================

Total trainable parameters: 2,038,617,088
Total (millions): 2038.62M

Memory Estimates (BF16=True):
  - Weights Only:              3.80 GB
  - Inference (bs=2, seq=128):  3.81 GB
       KV Cache:             0.01 GB
  - Training (AdamW):          18.99 GB
       Persistent (w+g+m+v): 15.19 GB
       Activations (est.):   3.80 GB
      Note: Activation memory is a rough lower bound. Real usage can be 2-4x higher
