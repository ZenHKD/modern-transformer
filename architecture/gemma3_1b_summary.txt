Modern Transformer Model Summary
================================================================================

Configuration:
  d_model: 1152
  n_layers: 26
  n_heads: 4
  n_kv_heads: 4
  d_ff: 11264
  max_seq_len: 131072
  sliding_window: None
  dropout: 0.0
  drop_path: 0.0
  init_std: 0.02
  use_bias: False
  num_experts: 1
  top_k: 1
  capacity_factor: 1.2
  num_shared_experts: 0
  aux_loss_coef: 0.01
  router_z_loss_coef: 0.001
  noise_std: 0.1

================================================================================

  architecture: decoder_only
  vocab_size: 256000
  weight_tying: True
  apply_rope: True
  rope_theta: 10000.0
  rope_fraction: 1.0
  rope_scaling: ntk
  rope_in_cross_attn: False
  qk_norm: True
  logit_cap: 30.0
  cache_mode: static
  num_sink_tokens: 4
  use_gradient_checkpointing: True
  use_bf16: True
  encoder: None
  decoder: BlockConfig(d_model=1152, n_layers=26, n_heads=4, n_kv_heads=4, d_ff=11264, max_seq_len=131072, sliding_window=None, dropout=0.0, drop_path=0.0, init_std=0.02, use_bias=False, num_experts=1, top_k=1, capacity_factor=1.2, num_shared_experts=0, aux_loss_coef=0.01, router_z_loss_coef=0.001, noise_std=0.1)

================================================================================

Architecture:
--------------------------------------------------------------------------------
TransformerModel(
  (embed_tokens): Embedding(256000, 1152)
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(256000, 1152)
    (layers): ModuleList(
      (0-25): 26 x DecoderBlock(
        (self_attention): AttentionBlock(
          (wq): Linear(in_features=1152, out_features=1152, bias=False)
          (wk): Linear(in_features=1152, out_features=1152, bias=False)
          (wv): Linear(in_features=1152, out_features=1152, bias=False)
          (wo): Linear(in_features=1152, out_features=1152, bias=False)
          (q_norm): RMSNorm()
          (k_norm): RMSNorm()
          (rope): RotaryPositionalEmbedding()
        )
        (self_attention_norm): RMSNorm()
        (cross_attention): AttentionBlock(
          (wq): Linear(in_features=1152, out_features=1152, bias=False)
          (wk): Linear(in_features=1152, out_features=1152, bias=False)
          (wv): Linear(in_features=1152, out_features=1152, bias=False)
          (wo): Linear(in_features=1152, out_features=1152, bias=False)
          (q_norm): RMSNorm()
          (k_norm): RMSNorm()
        )
        (cross_attention_norm): RMSNorm()
        (feed_forward): ExpertChoiceMoE(
          (gate): Linear(in_features=1152, out_features=1, bias=False)
          (experts): ModuleList(
            (0): SwiGLU(
              (w1): Linear(in_features=1152, out_features=11264, bias=False)
              (w2): Linear(in_features=1152, out_features=11264, bias=False)
              (w3): Linear(in_features=11264, out_features=1152, bias=False)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (shared_experts): ModuleList()
        )
        (ffn_norm): RMSNorm()
        (drop_path): Identity()
      )
    )
    (norm): RMSNorm()
  )
  (head): Linear(in_features=1152, out_features=256000, bias=False)
)

================================================================================

Trainable Parameters per Layer:
Layer Name                                                             |      Parameters
------------------------------------------------------------------------------------------
embed_tokens.weight                                                    |     294,912,000
decoder.layers.0.self_attention.wq.weight                              |       1,327,104
decoder.layers.0.self_attention.wk.weight                              |       1,327,104
decoder.layers.0.self_attention.wv.weight                              |       1,327,104
decoder.layers.0.self_attention.wo.weight                              |       1,327,104
decoder.layers.0.self_attention.q_norm.weight                          |             288
decoder.layers.0.self_attention.k_norm.weight                          |             288
decoder.layers.0.self_attention_norm.weight                            |           1,152
decoder.layers.0.cross_attention.wq.weight                             |       1,327,104
decoder.layers.0.cross_attention.wk.weight                             |       1,327,104
decoder.layers.0.cross_attention.wv.weight                             |       1,327,104
decoder.layers.0.cross_attention.wo.weight                             |       1,327,104
decoder.layers.0.cross_attention.q_norm.weight                         |             288
decoder.layers.0.cross_attention.k_norm.weight                         |             288
decoder.layers.0.cross_attention_norm.weight                           |           1,152
decoder.layers.0.feed_forward.gate.weight                              |           1,152
decoder.layers.0.feed_forward.experts.0.w1.weight                      |      12,976,128
decoder.layers.0.feed_forward.experts.0.w2.weight                      |      12,976,128
decoder.layers.0.feed_forward.experts.0.w3.weight                      |      12,976,128
decoder.layers.0.ffn_norm.weight                                       |           1,152
decoder.layers.1.self_attention.wq.weight                              |       1,327,104
decoder.layers.1.self_attention.wk.weight                              |       1,327,104
decoder.layers.1.self_attention.wv.weight                              |       1,327,104
decoder.layers.1.self_attention.wo.weight                              |       1,327,104
decoder.layers.1.self_attention.q_norm.weight                          |             288
decoder.layers.1.self_attention.k_norm.weight                          |             288
decoder.layers.1.self_attention_norm.weight                            |           1,152
decoder.layers.1.cross_attention.wq.weight                             |       1,327,104
decoder.layers.1.cross_attention.wk.weight                             |       1,327,104
decoder.layers.1.cross_attention.wv.weight                             |       1,327,104
decoder.layers.1.cross_attention.wo.weight                             |       1,327,104
decoder.layers.1.cross_attention.q_norm.weight                         |             288
decoder.layers.1.cross_attention.k_norm.weight                         |             288
decoder.layers.1.cross_attention_norm.weight                           |           1,152
decoder.layers.1.feed_forward.gate.weight                              |           1,152
decoder.layers.1.feed_forward.experts.0.w1.weight                      |      12,976,128
decoder.layers.1.feed_forward.experts.0.w2.weight                      |      12,976,128
decoder.layers.1.feed_forward.experts.0.w3.weight                      |      12,976,128
decoder.layers.1.ffn_norm.weight                                       |           1,152
decoder.layers.2.self_attention.wq.weight                              |       1,327,104
decoder.layers.2.self_attention.wk.weight                              |       1,327,104
decoder.layers.2.self_attention.wv.weight                              |       1,327,104
decoder.layers.2.self_attention.wo.weight                              |       1,327,104
decoder.layers.2.self_attention.q_norm.weight                          |             288
decoder.layers.2.self_attention.k_norm.weight                          |             288
decoder.layers.2.self_attention_norm.weight                            |           1,152
decoder.layers.2.cross_attention.wq.weight                             |       1,327,104
decoder.layers.2.cross_attention.wk.weight                             |       1,327,104
decoder.layers.2.cross_attention.wv.weight                             |       1,327,104
decoder.layers.2.cross_attention.wo.weight                             |       1,327,104
decoder.layers.2.cross_attention.q_norm.weight                         |             288
decoder.layers.2.cross_attention.k_norm.weight                         |             288
decoder.layers.2.cross_attention_norm.weight                           |           1,152
decoder.layers.2.feed_forward.gate.weight                              |           1,152
decoder.layers.2.feed_forward.experts.0.w1.weight                      |      12,976,128
decoder.layers.2.feed_forward.experts.0.w2.weight                      |      12,976,128
decoder.layers.2.feed_forward.experts.0.w3.weight                      |      12,976,128
decoder.layers.2.ffn_norm.weight                                       |           1,152
decoder.layers.3.self_attention.wq.weight                              |       1,327,104
decoder.layers.3.self_attention.wk.weight                              |       1,327,104
decoder.layers.3.self_attention.wv.weight                              |       1,327,104
decoder.layers.3.self_attention.wo.weight                              |       1,327,104
decoder.layers.3.self_attention.q_norm.weight                          |             288
decoder.layers.3.self_attention.k_norm.weight                          |             288
decoder.layers.3.self_attention_norm.weight                            |           1,152
decoder.layers.3.cross_attention.wq.weight                             |       1,327,104
decoder.layers.3.cross_attention.wk.weight                             |       1,327,104
decoder.layers.3.cross_attention.wv.weight                             |       1,327,104
decoder.layers.3.cross_attention.wo.weight                             |       1,327,104
decoder.layers.3.cross_attention.q_norm.weight                         |             288
decoder.layers.3.cross_attention.k_norm.weight                         |             288
decoder.layers.3.cross_attention_norm.weight                           |           1,152
decoder.layers.3.feed_forward.gate.weight                              |           1,152
decoder.layers.3.feed_forward.experts.0.w1.weight                      |      12,976,128
decoder.layers.3.feed_forward.experts.0.w2.weight                      |      12,976,128
decoder.layers.3.feed_forward.experts.0.w3.weight                      |      12,976,128
decoder.layers.3.ffn_norm.weight                                       |           1,152
decoder.layers.4.self_attention.wq.weight                              |       1,327,104
decoder.layers.4.self_attention.wk.weight                              |       1,327,104
decoder.layers.4.self_attention.wv.weight                              |       1,327,104
decoder.layers.4.self_attention.wo.weight                              |       1,327,104
decoder.layers.4.self_attention.q_norm.weight                          |             288
decoder.layers.4.self_attention.k_norm.weight                          |             288
decoder.layers.4.self_attention_norm.weight                            |           1,152
decoder.layers.4.cross_attention.wq.weight                             |       1,327,104
decoder.layers.4.cross_attention.wk.weight                             |       1,327,104
decoder.layers.4.cross_attention.wv.weight                             |       1,327,104
decoder.layers.4.cross_attention.wo.weight                             |       1,327,104
decoder.layers.4.cross_attention.q_norm.weight                         |             288
decoder.layers.4.cross_attention.k_norm.weight                         |             288
decoder.layers.4.cross_attention_norm.weight                           |           1,152
decoder.layers.4.feed_forward.gate.weight                              |           1,152
decoder.layers.4.feed_forward.experts.0.w1.weight                      |      12,976,128
decoder.layers.4.feed_forward.experts.0.w2.weight                      |      12,976,128
decoder.layers.4.feed_forward.experts.0.w3.weight                      |      12,976,128
decoder.layers.4.ffn_norm.weight                                       |           1,152
decoder.layers.5.self_attention.wq.weight                              |       1,327,104
decoder.layers.5.self_attention.wk.weight                              |       1,327,104
decoder.layers.5.self_attention.wv.weight                              |       1,327,104
decoder.layers.5.self_attention.wo.weight                              |       1,327,104
decoder.layers.5.self_attention.q_norm.weight                          |             288
decoder.layers.5.self_attention.k_norm.weight                          |             288
decoder.layers.5.self_attention_norm.weight                            |           1,152
decoder.layers.5.cross_attention.wq.weight                             |       1,327,104
decoder.layers.5.cross_attention.wk.weight                             |       1,327,104
decoder.layers.5.cross_attention.wv.weight                             |       1,327,104
decoder.layers.5.cross_attention.wo.weight                             |       1,327,104
decoder.layers.5.cross_attention.q_norm.weight                         |             288
decoder.layers.5.cross_attention.k_norm.weight                         |             288
decoder.layers.5.cross_attention_norm.weight                           |           1,152
decoder.layers.5.feed_forward.gate.weight                              |           1,152
decoder.layers.5.feed_forward.experts.0.w1.weight                      |      12,976,128
decoder.layers.5.feed_forward.experts.0.w2.weight                      |      12,976,128
decoder.layers.5.feed_forward.experts.0.w3.weight                      |      12,976,128
decoder.layers.5.ffn_norm.weight                                       |           1,152
decoder.layers.6.self_attention.wq.weight                              |       1,327,104
decoder.layers.6.self_attention.wk.weight                              |       1,327,104
decoder.layers.6.self_attention.wv.weight                              |       1,327,104
decoder.layers.6.self_attention.wo.weight                              |       1,327,104
decoder.layers.6.self_attention.q_norm.weight                          |             288
decoder.layers.6.self_attention.k_norm.weight                          |             288
decoder.layers.6.self_attention_norm.weight                            |           1,152
decoder.layers.6.cross_attention.wq.weight                             |       1,327,104
decoder.layers.6.cross_attention.wk.weight                             |       1,327,104
decoder.layers.6.cross_attention.wv.weight                             |       1,327,104
decoder.layers.6.cross_attention.wo.weight                             |       1,327,104
decoder.layers.6.cross_attention.q_norm.weight                         |             288
decoder.layers.6.cross_attention.k_norm.weight                         |             288
decoder.layers.6.cross_attention_norm.weight                           |           1,152
decoder.layers.6.feed_forward.gate.weight                              |           1,152
decoder.layers.6.feed_forward.experts.0.w1.weight                      |      12,976,128
decoder.layers.6.feed_forward.experts.0.w2.weight                      |      12,976,128
decoder.layers.6.feed_forward.experts.0.w3.weight                      |      12,976,128
decoder.layers.6.ffn_norm.weight                                       |           1,152
decoder.layers.7.self_attention.wq.weight                              |       1,327,104
decoder.layers.7.self_attention.wk.weight                              |       1,327,104
decoder.layers.7.self_attention.wv.weight                              |       1,327,104
decoder.layers.7.self_attention.wo.weight                              |       1,327,104
decoder.layers.7.self_attention.q_norm.weight                          |             288
decoder.layers.7.self_attention.k_norm.weight                          |             288
decoder.layers.7.self_attention_norm.weight                            |           1,152
decoder.layers.7.cross_attention.wq.weight                             |       1,327,104
decoder.layers.7.cross_attention.wk.weight                             |       1,327,104
decoder.layers.7.cross_attention.wv.weight                             |       1,327,104
decoder.layers.7.cross_attention.wo.weight                             |       1,327,104
decoder.layers.7.cross_attention.q_norm.weight                         |             288
decoder.layers.7.cross_attention.k_norm.weight                         |             288
decoder.layers.7.cross_attention_norm.weight                           |           1,152
decoder.layers.7.feed_forward.gate.weight                              |           1,152
decoder.layers.7.feed_forward.experts.0.w1.weight                      |      12,976,128
decoder.layers.7.feed_forward.experts.0.w2.weight                      |      12,976,128
decoder.layers.7.feed_forward.experts.0.w3.weight                      |      12,976,128
decoder.layers.7.ffn_norm.weight                                       |           1,152
decoder.layers.8.self_attention.wq.weight                              |       1,327,104
decoder.layers.8.self_attention.wk.weight                              |       1,327,104
decoder.layers.8.self_attention.wv.weight                              |       1,327,104
decoder.layers.8.self_attention.wo.weight                              |       1,327,104
decoder.layers.8.self_attention.q_norm.weight                          |             288
decoder.layers.8.self_attention.k_norm.weight                          |             288
decoder.layers.8.self_attention_norm.weight                            |           1,152
decoder.layers.8.cross_attention.wq.weight                             |       1,327,104
decoder.layers.8.cross_attention.wk.weight                             |       1,327,104
decoder.layers.8.cross_attention.wv.weight                             |       1,327,104
decoder.layers.8.cross_attention.wo.weight                             |       1,327,104
decoder.layers.8.cross_attention.q_norm.weight                         |             288
decoder.layers.8.cross_attention.k_norm.weight                         |             288
decoder.layers.8.cross_attention_norm.weight                           |           1,152
decoder.layers.8.feed_forward.gate.weight                              |           1,152
decoder.layers.8.feed_forward.experts.0.w1.weight                      |      12,976,128
decoder.layers.8.feed_forward.experts.0.w2.weight                      |      12,976,128
decoder.layers.8.feed_forward.experts.0.w3.weight                      |      12,976,128
decoder.layers.8.ffn_norm.weight                                       |           1,152
decoder.layers.9.self_attention.wq.weight                              |       1,327,104
decoder.layers.9.self_attention.wk.weight                              |       1,327,104
decoder.layers.9.self_attention.wv.weight                              |       1,327,104
decoder.layers.9.self_attention.wo.weight                              |       1,327,104
decoder.layers.9.self_attention.q_norm.weight                          |             288
decoder.layers.9.self_attention.k_norm.weight                          |             288
decoder.layers.9.self_attention_norm.weight                            |           1,152
decoder.layers.9.cross_attention.wq.weight                             |       1,327,104
decoder.layers.9.cross_attention.wk.weight                             |       1,327,104
decoder.layers.9.cross_attention.wv.weight                             |       1,327,104
decoder.layers.9.cross_attention.wo.weight                             |       1,327,104
decoder.layers.9.cross_attention.q_norm.weight                         |             288
decoder.layers.9.cross_attention.k_norm.weight                         |             288
decoder.layers.9.cross_attention_norm.weight                           |           1,152
decoder.layers.9.feed_forward.gate.weight                              |           1,152
decoder.layers.9.feed_forward.experts.0.w1.weight                      |      12,976,128
decoder.layers.9.feed_forward.experts.0.w2.weight                      |      12,976,128
decoder.layers.9.feed_forward.experts.0.w3.weight                      |      12,976,128
decoder.layers.9.ffn_norm.weight                                       |           1,152
decoder.layers.10.self_attention.wq.weight                             |       1,327,104
decoder.layers.10.self_attention.wk.weight                             |       1,327,104
decoder.layers.10.self_attention.wv.weight                             |       1,327,104
decoder.layers.10.self_attention.wo.weight                             |       1,327,104
decoder.layers.10.self_attention.q_norm.weight                         |             288
decoder.layers.10.self_attention.k_norm.weight                         |             288
decoder.layers.10.self_attention_norm.weight                           |           1,152
decoder.layers.10.cross_attention.wq.weight                            |       1,327,104
decoder.layers.10.cross_attention.wk.weight                            |       1,327,104
decoder.layers.10.cross_attention.wv.weight                            |       1,327,104
decoder.layers.10.cross_attention.wo.weight                            |       1,327,104
decoder.layers.10.cross_attention.q_norm.weight                        |             288
decoder.layers.10.cross_attention.k_norm.weight                        |             288
decoder.layers.10.cross_attention_norm.weight                          |           1,152
decoder.layers.10.feed_forward.gate.weight                             |           1,152
decoder.layers.10.feed_forward.experts.0.w1.weight                     |      12,976,128
decoder.layers.10.feed_forward.experts.0.w2.weight                     |      12,976,128
decoder.layers.10.feed_forward.experts.0.w3.weight                     |      12,976,128
decoder.layers.10.ffn_norm.weight                                      |           1,152
decoder.layers.11.self_attention.wq.weight                             |       1,327,104
decoder.layers.11.self_attention.wk.weight                             |       1,327,104
decoder.layers.11.self_attention.wv.weight                             |       1,327,104
decoder.layers.11.self_attention.wo.weight                             |       1,327,104
decoder.layers.11.self_attention.q_norm.weight                         |             288
decoder.layers.11.self_attention.k_norm.weight                         |             288
decoder.layers.11.self_attention_norm.weight                           |           1,152
decoder.layers.11.cross_attention.wq.weight                            |       1,327,104
decoder.layers.11.cross_attention.wk.weight                            |       1,327,104
decoder.layers.11.cross_attention.wv.weight                            |       1,327,104
decoder.layers.11.cross_attention.wo.weight                            |       1,327,104
decoder.layers.11.cross_attention.q_norm.weight                        |             288
decoder.layers.11.cross_attention.k_norm.weight                        |             288
decoder.layers.11.cross_attention_norm.weight                          |           1,152
decoder.layers.11.feed_forward.gate.weight                             |           1,152
decoder.layers.11.feed_forward.experts.0.w1.weight                     |      12,976,128
decoder.layers.11.feed_forward.experts.0.w2.weight                     |      12,976,128
decoder.layers.11.feed_forward.experts.0.w3.weight                     |      12,976,128
decoder.layers.11.ffn_norm.weight                                      |           1,152
decoder.layers.12.self_attention.wq.weight                             |       1,327,104
decoder.layers.12.self_attention.wk.weight                             |       1,327,104
decoder.layers.12.self_attention.wv.weight                             |       1,327,104
decoder.layers.12.self_attention.wo.weight                             |       1,327,104
decoder.layers.12.self_attention.q_norm.weight                         |             288
decoder.layers.12.self_attention.k_norm.weight                         |             288
decoder.layers.12.self_attention_norm.weight                           |           1,152
decoder.layers.12.cross_attention.wq.weight                            |       1,327,104
decoder.layers.12.cross_attention.wk.weight                            |       1,327,104
decoder.layers.12.cross_attention.wv.weight                            |       1,327,104
decoder.layers.12.cross_attention.wo.weight                            |       1,327,104
decoder.layers.12.cross_attention.q_norm.weight                        |             288
decoder.layers.12.cross_attention.k_norm.weight                        |             288
decoder.layers.12.cross_attention_norm.weight                          |           1,152
decoder.layers.12.feed_forward.gate.weight                             |           1,152
decoder.layers.12.feed_forward.experts.0.w1.weight                     |      12,976,128
decoder.layers.12.feed_forward.experts.0.w2.weight                     |      12,976,128
decoder.layers.12.feed_forward.experts.0.w3.weight                     |      12,976,128
decoder.layers.12.ffn_norm.weight                                      |           1,152
decoder.layers.13.self_attention.wq.weight                             |       1,327,104
decoder.layers.13.self_attention.wk.weight                             |       1,327,104
decoder.layers.13.self_attention.wv.weight                             |       1,327,104
decoder.layers.13.self_attention.wo.weight                             |       1,327,104
decoder.layers.13.self_attention.q_norm.weight                         |             288
decoder.layers.13.self_attention.k_norm.weight                         |             288
decoder.layers.13.self_attention_norm.weight                           |           1,152
decoder.layers.13.cross_attention.wq.weight                            |       1,327,104
decoder.layers.13.cross_attention.wk.weight                            |       1,327,104
decoder.layers.13.cross_attention.wv.weight                            |       1,327,104
decoder.layers.13.cross_attention.wo.weight                            |       1,327,104
decoder.layers.13.cross_attention.q_norm.weight                        |             288
decoder.layers.13.cross_attention.k_norm.weight                        |             288
decoder.layers.13.cross_attention_norm.weight                          |           1,152
decoder.layers.13.feed_forward.gate.weight                             |           1,152
decoder.layers.13.feed_forward.experts.0.w1.weight                     |      12,976,128
decoder.layers.13.feed_forward.experts.0.w2.weight                     |      12,976,128
decoder.layers.13.feed_forward.experts.0.w3.weight                     |      12,976,128
decoder.layers.13.ffn_norm.weight                                      |           1,152
decoder.layers.14.self_attention.wq.weight                             |       1,327,104
decoder.layers.14.self_attention.wk.weight                             |       1,327,104
decoder.layers.14.self_attention.wv.weight                             |       1,327,104
decoder.layers.14.self_attention.wo.weight                             |       1,327,104
decoder.layers.14.self_attention.q_norm.weight                         |             288
decoder.layers.14.self_attention.k_norm.weight                         |             288
decoder.layers.14.self_attention_norm.weight                           |           1,152
decoder.layers.14.cross_attention.wq.weight                            |       1,327,104
decoder.layers.14.cross_attention.wk.weight                            |       1,327,104
decoder.layers.14.cross_attention.wv.weight                            |       1,327,104
decoder.layers.14.cross_attention.wo.weight                            |       1,327,104
decoder.layers.14.cross_attention.q_norm.weight                        |             288
decoder.layers.14.cross_attention.k_norm.weight                        |             288
decoder.layers.14.cross_attention_norm.weight                          |           1,152
decoder.layers.14.feed_forward.gate.weight                             |           1,152
decoder.layers.14.feed_forward.experts.0.w1.weight                     |      12,976,128
decoder.layers.14.feed_forward.experts.0.w2.weight                     |      12,976,128
decoder.layers.14.feed_forward.experts.0.w3.weight                     |      12,976,128
decoder.layers.14.ffn_norm.weight                                      |           1,152
decoder.layers.15.self_attention.wq.weight                             |       1,327,104
decoder.layers.15.self_attention.wk.weight                             |       1,327,104
decoder.layers.15.self_attention.wv.weight                             |       1,327,104
decoder.layers.15.self_attention.wo.weight                             |       1,327,104
decoder.layers.15.self_attention.q_norm.weight                         |             288
decoder.layers.15.self_attention.k_norm.weight                         |             288
decoder.layers.15.self_attention_norm.weight                           |           1,152
decoder.layers.15.cross_attention.wq.weight                            |       1,327,104
decoder.layers.15.cross_attention.wk.weight                            |       1,327,104
decoder.layers.15.cross_attention.wv.weight                            |       1,327,104
decoder.layers.15.cross_attention.wo.weight                            |       1,327,104
decoder.layers.15.cross_attention.q_norm.weight                        |             288
decoder.layers.15.cross_attention.k_norm.weight                        |             288
decoder.layers.15.cross_attention_norm.weight                          |           1,152
decoder.layers.15.feed_forward.gate.weight                             |           1,152
decoder.layers.15.feed_forward.experts.0.w1.weight                     |      12,976,128
decoder.layers.15.feed_forward.experts.0.w2.weight                     |      12,976,128
decoder.layers.15.feed_forward.experts.0.w3.weight                     |      12,976,128
decoder.layers.15.ffn_norm.weight                                      |           1,152
decoder.layers.16.self_attention.wq.weight                             |       1,327,104
decoder.layers.16.self_attention.wk.weight                             |       1,327,104
decoder.layers.16.self_attention.wv.weight                             |       1,327,104
decoder.layers.16.self_attention.wo.weight                             |       1,327,104
decoder.layers.16.self_attention.q_norm.weight                         |             288
decoder.layers.16.self_attention.k_norm.weight                         |             288
decoder.layers.16.self_attention_norm.weight                           |           1,152
decoder.layers.16.cross_attention.wq.weight                            |       1,327,104
decoder.layers.16.cross_attention.wk.weight                            |       1,327,104
decoder.layers.16.cross_attention.wv.weight                            |       1,327,104
decoder.layers.16.cross_attention.wo.weight                            |       1,327,104
decoder.layers.16.cross_attention.q_norm.weight                        |             288
decoder.layers.16.cross_attention.k_norm.weight                        |             288
decoder.layers.16.cross_attention_norm.weight                          |           1,152
decoder.layers.16.feed_forward.gate.weight                             |           1,152
decoder.layers.16.feed_forward.experts.0.w1.weight                     |      12,976,128
decoder.layers.16.feed_forward.experts.0.w2.weight                     |      12,976,128
decoder.layers.16.feed_forward.experts.0.w3.weight                     |      12,976,128
decoder.layers.16.ffn_norm.weight                                      |           1,152
decoder.layers.17.self_attention.wq.weight                             |       1,327,104
decoder.layers.17.self_attention.wk.weight                             |       1,327,104
decoder.layers.17.self_attention.wv.weight                             |       1,327,104
decoder.layers.17.self_attention.wo.weight                             |       1,327,104
decoder.layers.17.self_attention.q_norm.weight                         |             288
decoder.layers.17.self_attention.k_norm.weight                         |             288
decoder.layers.17.self_attention_norm.weight                           |           1,152
decoder.layers.17.cross_attention.wq.weight                            |       1,327,104
decoder.layers.17.cross_attention.wk.weight                            |       1,327,104
decoder.layers.17.cross_attention.wv.weight                            |       1,327,104
decoder.layers.17.cross_attention.wo.weight                            |       1,327,104
decoder.layers.17.cross_attention.q_norm.weight                        |             288
decoder.layers.17.cross_attention.k_norm.weight                        |             288
decoder.layers.17.cross_attention_norm.weight                          |           1,152
decoder.layers.17.feed_forward.gate.weight                             |           1,152
decoder.layers.17.feed_forward.experts.0.w1.weight                     |      12,976,128
decoder.layers.17.feed_forward.experts.0.w2.weight                     |      12,976,128
decoder.layers.17.feed_forward.experts.0.w3.weight                     |      12,976,128
decoder.layers.17.ffn_norm.weight                                      |           1,152
decoder.layers.18.self_attention.wq.weight                             |       1,327,104
decoder.layers.18.self_attention.wk.weight                             |       1,327,104
decoder.layers.18.self_attention.wv.weight                             |       1,327,104
decoder.layers.18.self_attention.wo.weight                             |       1,327,104
decoder.layers.18.self_attention.q_norm.weight                         |             288
decoder.layers.18.self_attention.k_norm.weight                         |             288
decoder.layers.18.self_attention_norm.weight                           |           1,152
decoder.layers.18.cross_attention.wq.weight                            |       1,327,104
decoder.layers.18.cross_attention.wk.weight                            |       1,327,104
decoder.layers.18.cross_attention.wv.weight                            |       1,327,104
decoder.layers.18.cross_attention.wo.weight                            |       1,327,104
decoder.layers.18.cross_attention.q_norm.weight                        |             288
decoder.layers.18.cross_attention.k_norm.weight                        |             288
decoder.layers.18.cross_attention_norm.weight                          |           1,152
decoder.layers.18.feed_forward.gate.weight                             |           1,152
decoder.layers.18.feed_forward.experts.0.w1.weight                     |      12,976,128
decoder.layers.18.feed_forward.experts.0.w2.weight                     |      12,976,128
decoder.layers.18.feed_forward.experts.0.w3.weight                     |      12,976,128
decoder.layers.18.ffn_norm.weight                                      |           1,152
decoder.layers.19.self_attention.wq.weight                             |       1,327,104
decoder.layers.19.self_attention.wk.weight                             |       1,327,104
decoder.layers.19.self_attention.wv.weight                             |       1,327,104
decoder.layers.19.self_attention.wo.weight                             |       1,327,104
decoder.layers.19.self_attention.q_norm.weight                         |             288
decoder.layers.19.self_attention.k_norm.weight                         |             288
decoder.layers.19.self_attention_norm.weight                           |           1,152
decoder.layers.19.cross_attention.wq.weight                            |       1,327,104
decoder.layers.19.cross_attention.wk.weight                            |       1,327,104
decoder.layers.19.cross_attention.wv.weight                            |       1,327,104
decoder.layers.19.cross_attention.wo.weight                            |       1,327,104
decoder.layers.19.cross_attention.q_norm.weight                        |             288
decoder.layers.19.cross_attention.k_norm.weight                        |             288
decoder.layers.19.cross_attention_norm.weight                          |           1,152
decoder.layers.19.feed_forward.gate.weight                             |           1,152
decoder.layers.19.feed_forward.experts.0.w1.weight                     |      12,976,128
decoder.layers.19.feed_forward.experts.0.w2.weight                     |      12,976,128
decoder.layers.19.feed_forward.experts.0.w3.weight                     |      12,976,128
decoder.layers.19.ffn_norm.weight                                      |           1,152
decoder.layers.20.self_attention.wq.weight                             |       1,327,104
decoder.layers.20.self_attention.wk.weight                             |       1,327,104
decoder.layers.20.self_attention.wv.weight                             |       1,327,104
decoder.layers.20.self_attention.wo.weight                             |       1,327,104
decoder.layers.20.self_attention.q_norm.weight                         |             288
decoder.layers.20.self_attention.k_norm.weight                         |             288
decoder.layers.20.self_attention_norm.weight                           |           1,152
decoder.layers.20.cross_attention.wq.weight                            |       1,327,104
decoder.layers.20.cross_attention.wk.weight                            |       1,327,104
decoder.layers.20.cross_attention.wv.weight                            |       1,327,104
decoder.layers.20.cross_attention.wo.weight                            |       1,327,104
decoder.layers.20.cross_attention.q_norm.weight                        |             288
decoder.layers.20.cross_attention.k_norm.weight                        |             288
decoder.layers.20.cross_attention_norm.weight                          |           1,152
decoder.layers.20.feed_forward.gate.weight                             |           1,152
decoder.layers.20.feed_forward.experts.0.w1.weight                     |      12,976,128
decoder.layers.20.feed_forward.experts.0.w2.weight                     |      12,976,128
decoder.layers.20.feed_forward.experts.0.w3.weight                     |      12,976,128
decoder.layers.20.ffn_norm.weight                                      |           1,152
decoder.layers.21.self_attention.wq.weight                             |       1,327,104
decoder.layers.21.self_attention.wk.weight                             |       1,327,104
decoder.layers.21.self_attention.wv.weight                             |       1,327,104
decoder.layers.21.self_attention.wo.weight                             |       1,327,104
decoder.layers.21.self_attention.q_norm.weight                         |             288
decoder.layers.21.self_attention.k_norm.weight                         |             288
decoder.layers.21.self_attention_norm.weight                           |           1,152
decoder.layers.21.cross_attention.wq.weight                            |       1,327,104
decoder.layers.21.cross_attention.wk.weight                            |       1,327,104
decoder.layers.21.cross_attention.wv.weight                            |       1,327,104
decoder.layers.21.cross_attention.wo.weight                            |       1,327,104
decoder.layers.21.cross_attention.q_norm.weight                        |             288
decoder.layers.21.cross_attention.k_norm.weight                        |             288
decoder.layers.21.cross_attention_norm.weight                          |           1,152
decoder.layers.21.feed_forward.gate.weight                             |           1,152
decoder.layers.21.feed_forward.experts.0.w1.weight                     |      12,976,128
decoder.layers.21.feed_forward.experts.0.w2.weight                     |      12,976,128
decoder.layers.21.feed_forward.experts.0.w3.weight                     |      12,976,128
decoder.layers.21.ffn_norm.weight                                      |           1,152
decoder.layers.22.self_attention.wq.weight                             |       1,327,104
decoder.layers.22.self_attention.wk.weight                             |       1,327,104
decoder.layers.22.self_attention.wv.weight                             |       1,327,104
decoder.layers.22.self_attention.wo.weight                             |       1,327,104
decoder.layers.22.self_attention.q_norm.weight                         |             288
decoder.layers.22.self_attention.k_norm.weight                         |             288
decoder.layers.22.self_attention_norm.weight                           |           1,152
decoder.layers.22.cross_attention.wq.weight                            |       1,327,104
decoder.layers.22.cross_attention.wk.weight                            |       1,327,104
decoder.layers.22.cross_attention.wv.weight                            |       1,327,104
decoder.layers.22.cross_attention.wo.weight                            |       1,327,104
decoder.layers.22.cross_attention.q_norm.weight                        |             288
decoder.layers.22.cross_attention.k_norm.weight                        |             288
decoder.layers.22.cross_attention_norm.weight                          |           1,152
decoder.layers.22.feed_forward.gate.weight                             |           1,152
decoder.layers.22.feed_forward.experts.0.w1.weight                     |      12,976,128
decoder.layers.22.feed_forward.experts.0.w2.weight                     |      12,976,128
decoder.layers.22.feed_forward.experts.0.w3.weight                     |      12,976,128
decoder.layers.22.ffn_norm.weight                                      |           1,152
decoder.layers.23.self_attention.wq.weight                             |       1,327,104
decoder.layers.23.self_attention.wk.weight                             |       1,327,104
decoder.layers.23.self_attention.wv.weight                             |       1,327,104
decoder.layers.23.self_attention.wo.weight                             |       1,327,104
decoder.layers.23.self_attention.q_norm.weight                         |             288
decoder.layers.23.self_attention.k_norm.weight                         |             288
decoder.layers.23.self_attention_norm.weight                           |           1,152
decoder.layers.23.cross_attention.wq.weight                            |       1,327,104
decoder.layers.23.cross_attention.wk.weight                            |       1,327,104
decoder.layers.23.cross_attention.wv.weight                            |       1,327,104
decoder.layers.23.cross_attention.wo.weight                            |       1,327,104
decoder.layers.23.cross_attention.q_norm.weight                        |             288
decoder.layers.23.cross_attention.k_norm.weight                        |             288
decoder.layers.23.cross_attention_norm.weight                          |           1,152
decoder.layers.23.feed_forward.gate.weight                             |           1,152
decoder.layers.23.feed_forward.experts.0.w1.weight                     |      12,976,128
decoder.layers.23.feed_forward.experts.0.w2.weight                     |      12,976,128
decoder.layers.23.feed_forward.experts.0.w3.weight                     |      12,976,128
decoder.layers.23.ffn_norm.weight                                      |           1,152
decoder.layers.24.self_attention.wq.weight                             |       1,327,104
decoder.layers.24.self_attention.wk.weight                             |       1,327,104
decoder.layers.24.self_attention.wv.weight                             |       1,327,104
decoder.layers.24.self_attention.wo.weight                             |       1,327,104
decoder.layers.24.self_attention.q_norm.weight                         |             288
decoder.layers.24.self_attention.k_norm.weight                         |             288
decoder.layers.24.self_attention_norm.weight                           |           1,152
decoder.layers.24.cross_attention.wq.weight                            |       1,327,104
decoder.layers.24.cross_attention.wk.weight                            |       1,327,104
decoder.layers.24.cross_attention.wv.weight                            |       1,327,104
decoder.layers.24.cross_attention.wo.weight                            |       1,327,104
decoder.layers.24.cross_attention.q_norm.weight                        |             288
decoder.layers.24.cross_attention.k_norm.weight                        |             288
decoder.layers.24.cross_attention_norm.weight                          |           1,152
decoder.layers.24.feed_forward.gate.weight                             |           1,152
decoder.layers.24.feed_forward.experts.0.w1.weight                     |      12,976,128
decoder.layers.24.feed_forward.experts.0.w2.weight                     |      12,976,128
decoder.layers.24.feed_forward.experts.0.w3.weight                     |      12,976,128
decoder.layers.24.ffn_norm.weight                                      |           1,152
decoder.layers.25.self_attention.wq.weight                             |       1,327,104
decoder.layers.25.self_attention.wk.weight                             |       1,327,104
decoder.layers.25.self_attention.wv.weight                             |       1,327,104
decoder.layers.25.self_attention.wo.weight                             |       1,327,104
decoder.layers.25.self_attention.q_norm.weight                         |             288
decoder.layers.25.self_attention.k_norm.weight                         |             288
decoder.layers.25.self_attention_norm.weight                           |           1,152
decoder.layers.25.cross_attention.wq.weight                            |       1,327,104
decoder.layers.25.cross_attention.wk.weight                            |       1,327,104
decoder.layers.25.cross_attention.wv.weight                            |       1,327,104
decoder.layers.25.cross_attention.wo.weight                            |       1,327,104
decoder.layers.25.cross_attention.q_norm.weight                        |             288
decoder.layers.25.cross_attention.k_norm.weight                        |             288
decoder.layers.25.cross_attention_norm.weight                          |           1,152
decoder.layers.25.feed_forward.gate.weight                             |           1,152
decoder.layers.25.feed_forward.experts.0.w1.weight                     |      12,976,128
decoder.layers.25.feed_forward.experts.0.w2.weight                     |      12,976,128
decoder.layers.25.feed_forward.experts.0.w3.weight                     |      12,976,128
decoder.layers.25.ffn_norm.weight                                      |           1,152
decoder.norm.weight                                                    |           1,152
==========================================================================================

Total trainable parameters: 1,288,326,528
Total (millions): 1288.33M

Memory Estimates (BF16=True):
  - Weights Only:              2.40 GB
  - Inference (bs=2, seq=128):  2.43 GB
      └─ KV Cache:             0.03 GB
  - Training (AdamW):          12.00 GB
      └─ Persistent (w+g+m+v): 9.60 GB
      └─ Activations (est.):   2.40 GB
      Note: Activation memory is a rough lower bound. Real usage can be 2-4x higher
